from __future__ import annotations

import argparse
import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Annotated, Mapping, Sequence, TypedDict

import requests
from dotenv import load_dotenv

# NOTE: langchain.agents.create_agent (v1.2.9+) å·²å…§å»º tool loopï¼Œ
# æœƒè‡ªå‹•è™•ç† tool calls ç›´åˆ° LLM åœæ­¢å‘¼å« tools
from langchain.agents import create_agent
from langchain.tools import tool
from langchain_community.agent_toolkits import FileManagementToolkit
from langchain_core.messages import BaseMessage, HumanMessage

# from langchain_google_vertexai import ChatVertexAI
from langchain_google_genai import ChatGoogleGenerativeAI

# NOTE: ç§»é™¤æœªä½¿ç”¨çš„ StateGraph/END importï¼Œå› ç‚º create_agent å·²ç¶“è¿”å› compiled graph
# ä¿ç•™ add_messages ä¾› AgentState ä½¿ç”¨
from langgraph.graph.message import add_messages

try:
    import yaml
except ImportError as exc:  # pragma: no cover
    raise ImportError("Please install PyYAML: pip install pyyaml") from exc
import sys


def ensure_repo_root_on_path() -> Path:
    """ç¢ºä¿ repo root å·²åŠ å…¥ sys.pathã€‚

    Returns:
        repo root è·¯å¾‘ã€‚
    """
    repo_root = Path(__file__).resolve().parents[1]
    if str(repo_root) not in sys.path:
        sys.path.append(str(repo_root))
    return repo_root


ensure_repo_root_on_path()
# ruff: noqa: F401
from runner.test_gen.pipeline_tool import generate_test  # noqa

# =========================
# Token Estimation Logic
# =========================


def estimate_tokens(file_path: str) -> int:
    """
    é€éæª”æ¡ˆå¤§å°ç²—ç•¥ä¼°è¨ˆ Token æ•¸é‡ã€‚
    ä¼°ç®—æ¨™æº–ï¼š1 Token â‰ˆ 4 Bytes (é©ç”¨æ–¼è‹±æ–‡/ç¨‹å¼ç¢¼/JSON)
    """
    try:
        if not os.path.exists(file_path):
            return 0
        file_size_bytes = os.path.getsize(file_path)
        return int(file_size_bytes / 4)
    except Exception:
        return 0


# =========================
# Data models
# =========================


@dataclass(frozen=True)
class LlmConfig:
    """LLM configuration for a ChatVertexAI model."""

    model: str
    project: str
    location: str
    temperature: float = 0.0


@dataclass(frozen=True)
class PromptConfig:
    """Prompt file paths."""

    architect_path: Path
    engineer_path: Path


@dataclass(frozen=True)
class AppConfig:
    """Top-level application configuration."""

    working_directory: Path
    ingest_url: str
    repo_url: str
    architect: LlmConfig
    engineer: LlmConfig
    prompts: PromptConfig
    source_dir: str
    target_dir: str
    repo_dir: str
    user_input_template: str
    log_filename: str = "multi_agent.log"


class AgentState(TypedDict):
    """LangGraph state containing the conversation history."""

    messages: Annotated[list[BaseMessage], add_messages]


# =========================
# Logging ("logger packer")
# =========================


class LogPacker:
    """Thin logging wrapper with a consistent, hackathon-friendly format.

    This intentionally stays minimal: file + console handlers, and a few
    helper methods for structured logging.
    """

    def __init__(self, log_path: Path) -> None:
        import logging

        self._logger = logging.getLogger("multi_agent")
        self._logger.setLevel(logging.INFO)
        self._logger.propagate = False

        # Avoid duplicate handlers if re-imported (e.g., notebooks).
        if self._logger.handlers:
            return

        log_path.parent.mkdir(parents=True, exist_ok=True)

        fmt = logging.Formatter(
            fmt="%(asctime)s [%(levelname)s] %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )

        file_handler = logging.FileHandler(log_path, encoding="utf-8")
        file_handler.setFormatter(fmt)
        self._logger.addHandler(file_handler)

        console_handler = logging.StreamHandler()
        console_handler.setFormatter(fmt)
        self._logger.addHandler(console_handler)

    def info(self, msg: str) -> None:
        """Logs an informational message."""

        self._logger.info(msg)

    def warning(self, msg: str) -> None:
        """Logs a warning message."""

        self._logger.warning(msg)

    def error(self, msg: str) -> None:
        """Logs an error message."""

        self._logger.error(msg)

    def json(self, label: str, payload: Mapping[str, object]) -> None:
        """Logs a JSON-serializable payload in a single line."""

        self._logger.info("%s: %s", label, json.dumps(payload, ensure_ascii=False))


# =========================
# Config / prompts
# =========================


def load_yaml_config(config_path: Path) -> dict:
    """Loads YAML config from disk.

    Args:
        config_path: Path to a YAML file.

    Returns:
        Parsed config as a dictionary.

    Raises:
        FileNotFoundError: If the config file does not exist.
        ValueError: If the YAML is empty or invalid.
    """

    if not config_path.exists():
        raise FileNotFoundError(f"Config not found: {config_path}")
    with config_path.open("r", encoding="utf-8") as f:
        data = yaml.safe_load(f)
    if not isinstance(data, dict):
        raise ValueError(f"Invalid YAML config (expected mapping): {config_path}")
    return data


def _as_path(base_dir: Path, raw: str) -> Path:
    """Resolves a possibly-relative path against a base directory."""

    p = Path(raw)
    return p if p.is_absolute() else (base_dir / p)


def parse_app_config(config_path: Path) -> AppConfig:
    """Parses an AppConfig from a YAML file.

    Args:
        config_path: Path to the YAML config file.

    Returns:
        AppConfig instance.
    """

    raw = load_yaml_config(config_path)
    base_dir = config_path.parent

    working_directory = _as_path(
        base_dir, str(raw.get("working_directory", "."))
    ).resolve()
    ingest_url = str(raw.get("ingest_url", "http://localhost:8000/ingestion/runs"))
    repo_url = str(
        raw.get("repo_url", "https://github.com/emilybache/Racing-Car-Katas.git")
    )

    architect_raw = (
        raw.get("llm", {}).get("architect", {})
        if isinstance(raw.get("llm"), dict)
        else {}
    )
    engineer_raw = (
        raw.get("llm", {}).get("engineer", {})
        if isinstance(raw.get("llm"), dict)
        else {}
    )

    prompts_raw = raw.get("prompts", {}) if isinstance(raw.get("prompts"), dict) else {}

    architect = LlmConfig(
        model=str(architect_raw.get("model", "qwen/qwen3-next-80b-a3b-thinking-maas")),
        project=str(architect_raw.get("project", "tsmchaker")),
        location=str(architect_raw.get("location", "global")),
        temperature=float(architect_raw.get("temperature", 0.0)),
    )
    engineer = LlmConfig(
        model=str(engineer_raw.get("model", "gemini-2.5-pro")),
        project=str(engineer_raw.get("project", "tsmchaker")),
        location=str(engineer_raw.get("location", "global")),
        temperature=float(engineer_raw.get("temperature", 0.0)),
    )

    prompts = PromptConfig(
        architect_path=_as_path(
            base_dir, str(prompts_raw.get("architect", "prompts/architect.md"))
        ),
        engineer_path=_as_path(
            base_dir, str(prompts_raw.get("engineer", "prompts/engineer.md"))
        ),
    )

    source_dir = str(raw.get("source_dir", "./Racing-Car-Katas/Python"))
    target_dir = str(raw.get("target_dir", "./refactor-golang"))
    repo_dir = str(raw.get("repo_dir", "./artifacts/caa0ea0651474be18c2f4c265c32b9eb"))
    user_input_template = str(
        raw.get(
            "user_input_template",
            "Refactor the codebase located in {working_directory}/{source_dir} "
            "into Golang. "
            "Output the new code to the "
            "{working_directory}/{target_dir} directory. "
            "Requirement: "
            "Preserve the exact business logic "
            "and include meaningful comments.",
        )
    )
    log_filename = str(raw.get("log_filename", "multi_agent.log"))

    return AppConfig(
        working_directory=working_directory,
        ingest_url=ingest_url,
        repo_url=repo_url,
        architect=architect,
        engineer=engineer,
        prompts=prompts,
        source_dir=source_dir,
        target_dir=target_dir,
        repo_dir=repo_dir,
        user_input_template=user_input_template,
        log_filename=log_filename,
    )


def load_prompt(prompt_path: Path) -> str:
    """Loads a prompt file from disk.

    Args:
        prompt_path: Path to a UTF-8 text prompt file (recommended: .md).

    Returns:
        Prompt content as a string.
    """

    return prompt_path.read_text(encoding="utf-8").strip()


def render_user_input(cfg: AppConfig) -> str:
    """Renders the user input prompt from template + YAML parameters."""

    return cfg.user_input_template.format(
        source_dir=cfg.source_dir.lstrip("./"),
        target_dir=cfg.target_dir.lstrip("./"),
        repo_dir=cfg.repo_dir.lstrip("./"),
    )


# =========================
# Initialization
# =========================


def init_file_management_tools(cfg: AppConfig, log: LogPacker):
    """
    Initializes file system tools with Wrappers:
    1. Read: Checks token limits (Safety).
    2. Write: Logs the file path (Transparency).
    """
    log.info(f"Initializing file system tools. root_dir={cfg.working_directory}")

    # 1. å–å¾—åŸå» æ¨™æº–å·¥å…·
    toolkit = FileManagementToolkit(root_dir=str(cfg.working_directory))
    std_tools = toolkit.get_tools()

    # ============================
    # Wrapper 1: Safe Read File
    # ============================
    original_read_tool = next((t for t in std_tools if t.name == "read_file"), None)

    # é è¨­ä½¿ç”¨åŸå» å·¥å…·ï¼Œç¨å¾Œå¦‚æœæœ‰å®šç¾© wrapper å‰‡æ›¿æ›
    final_read_tool = original_read_tool

    if original_read_tool:

        @tool("read_file")
        def safe_read_wrapper(file_path: str) -> str:
            """
            Read a file from the filesystem.
            Input: file_path (str)
            (Wrapper: Checks size first, then delegates to standard tool or truncates)
            """
            try:
                # A. è¨ˆç®—çµ•å°è·¯å¾‘
                target_path = (cfg.working_directory / file_path).resolve()

                # å®‰å…¨æª¢æŸ¥
                if not str(target_path).startswith(
                    str(cfg.working_directory.resolve())
                ):
                    return f"Error: Access denied. Path {file_path} \
                    is outside the working directory."

                if not target_path.exists():
                    return f"Error: File {file_path} does not exist."

                # B. æª¢æŸ¥å¤§å° (Token ä¼°ç®—)
                est_tokens = estimate_tokens(str(target_path))
                MAX_TOKENS = 300000

                if est_tokens > MAX_TOKENS:
                    read_chars = MAX_TOKENS * 4
                    log.warning(
                        f"ğŸ›¡ï¸ [SafeGuard] Intercepted large file:\
                         {file_path} (~{est_tokens} tokens). Truncating."
                    )
                    with open(
                        target_path, "r", encoding="utf-8", errors="replace"
                    ) as f:
                        preview = f.read(read_chars)
                    return (
                        f"{preview}\n\n"
                        f"====================================================\n"
                        f"[SYSTEM WARNING] File content truncated.\n"
                        f"Original size: ~{est_tokens} tokens. Limit: {MAX_TOKENS}.\n"
                        f"===================================================="
                    )

                # D. å‘¼å«åŸå» å·¥å…·
                log.info(f"ğŸ“„ [Read File] Reading {file_path} (~{est_tokens} tokens)")
                return original_read_tool.invoke(file_path)

            except Exception as e:
                error_msg = f"Error in safe_read_wrapper: {e}"
                log.error(error_msg)
                return error_msg

        final_read_tool = safe_read_wrapper

    # ============================
    # Wrapper 2: Log Write File  <-- æ–°å¢çš„éƒ¨åˆ†
    # ============================
    original_write_tool = next((t for t in std_tools if t.name == "write_file"), None)

    final_write_tool = original_write_tool

    if original_write_tool:

        @tool("write_file")
        def write_log_wrapper(file_path: str, text: str) -> str:
            """
            Write a file to the filesystem.
            Input: file_path (str), text (str) - The content to write.
            (Wrapper: Logs the write action with path)
            """
            # åœ¨é€™è£¡åŠ ä¸Š Log
            log.info(f"ğŸ’¾ [Write File] Saving content to: {file_path}")

            try:
                # å‘¼å«åŸå» å·¥å…·åŸ·è¡ŒçœŸæ­£çš„å¯«å…¥
                # æ³¨æ„: write_file é€šå¸¸éœ€è¦å‚³å…¥å­—å…¸åƒæ•¸
                return original_write_tool.invoke(
                    {"file_path": file_path, "text": text}
                )
            except Exception as e:
                error_msg = f"Error writing file {file_path}: {e}"
                log.error(f"âŒ [Write File] Failed: {error_msg}")
                return error_msg

        final_write_tool = write_log_wrapper

    # ============================
    # 3. çµ„è£æœ€çµ‚å·¥å…·åˆ—è¡¨
    # ============================
    final_tools = []
    for t in std_tools:
        if t.name == "read_file":
            if final_read_tool:
                final_tools.append(final_read_tool)
        elif t.name == "write_file":
            if final_write_tool:
                final_tools.append(final_write_tool)
        else:
            final_tools.append(t)

    return final_tools


def init_llms(cfg: AppConfig, log: LogPacker):
    """Initializes architect/engineer LLMs."""

    log.info("Initializing architect LLM (Architect)...")
    llm_architect = ChatGoogleGenerativeAI(
        model=cfg.architect.model,
        project=cfg.architect.project,
        # location=cfg.architect.location,
        temperature=cfg.architect.temperature,
    )

    log.info("Initializing engineer LLM (Engineer)...")
    llm_engineer = ChatGoogleGenerativeAI(
        model=cfg.engineer.model,
        project=cfg.engineer.project,
        # location=cfg.engineer.location,
        temperature=cfg.engineer.temperature,
    )

    # llm_architect = ChatGoogleGenerativeAI(
    #     model="gemini-2.5-pro",
    #     google_api_key=os.getenv("GOOGLE_API_KEY", "YOUR_KEY_HERE"),
    #     temperature=0,
    # )
    # llm_engineer = ChatGoogleGenerativeAI(
    #     model="gemini-2.5-flash",
    #     google_api_key=os.getenv("GOOGLE_API_KEY", "YOUR_KEY_HERE"),
    #     temperature=0,
    # )

    return llm_architect, llm_engineer


def build_graph(
    cfg: AppConfig,
    tools,
    llm_architect: ChatGoogleGenerativeAI,
    llm_engineer: ChatGoogleGenerativeAI,
    log: LogPacker,
):
    """Builds and compiles the LangGraph workflow.

    æ¶æ§‹èªªæ˜ï¼š
    - create_agent() å·²å…§å»º tool loopï¼ˆæœƒè‡ªå‹•è¿­ä»£åŸ·è¡Œ tool calls ç›´åˆ° LLM åœæ­¢ï¼‰
    - Architect æ˜¯ä¸»è¦ agentï¼Œè² è²¬è¦åŠƒå’Œå”èª¿
    - Engineer é€é refactor_code tool è¢« Architect å‘¼å«ï¼ŒåŸ·è¡Œå¯¦éš›é‡æ§‹
    - generate_test tool ç”¨æ–¼åŸ·è¡Œ characterization testing pipeline

    å·¥ä½œæµç¨‹ï¼š
    1. Architect è®€å– dependency graph å’Œ codebase
    2. Architect å»ºç«‹ spec.md è¦åŠƒæ–‡ä»¶
    3. Architect å‘¼å« refactor_code() å§”æ´¾é‡æ§‹ä»»å‹™çµ¦ Engineer
    4. Engineer ä½¿ç”¨ file tools åŸ·è¡Œé‡æ§‹
    5. Architect å‘¼å« generate_test() åŸ·è¡Œæ¸¬è©¦
    6. é‡è¤‡ 3-5 ç›´åˆ°æ‰€æœ‰ stage å®Œæˆ
    """

    architect_prompt_raw = load_prompt(cfg.prompts.architect_path)
    engineer_prompt_raw = load_prompt(cfg.prompts.engineer_path)

    engineer_system_prompt = engineer_prompt_raw.format(
        # working_directory=str(cfg.working_directory).rstrip("/"),
        target_dir=cfg.target_dir.lstrip("./"),
        # å¦‚æœéœ€è¦ source_dir æˆ– repo_dir ä¹Ÿå¯ä»¥åŠ é€²ä¾†
        # source_dir=cfg.source_dir,
    )

    architect_system_prompt = architect_prompt_raw.format(
        # working_directory=str(cfg.working_directory).rstrip("/"),
        # repo_dir=cfg.repo_dir.lstrip("./"),
        # å¦‚æœéœ€è¦ source_dir æˆ– repo_dir ä¹Ÿå¯ä»¥åŠ é€²ä¾†
        source_dir=cfg.source_dir,
    )

    # NOTE: Engineer agent - create_agent è¿”å›çš„æ˜¯å·²ç·¨è­¯çš„ graphï¼Œ
    # å…§éƒ¨å·²è™•ç† tool loopï¼ˆLLM â†’ tool call â†’ tool result â†’ LLM â†’ ...ï¼‰
    llm_engineer_with_tools = create_agent(
        llm_engineer,
        tools=tools,
        system_prompt=engineer_system_prompt,
    )

    @tool
    def refactor_code(request: str) -> str:
        """
        Refactor or modify existing code based on natural language instructions.

        Use this tool when the user requests structural changes,
        performance optimization, or logic modifications to their codebase.
        It acts as a bridge between high-level
        intent and technical code execution.

        Input: Natural language refactor code request
        (e.g., 'refactor code in the ./python to JAVA')
        """
        log.info(
            f"â¡ï¸ [Next Step] Architect is delegating "
            f"task to Engineer. Request: {request[:100]}..."
        )

        # NOTE: config æ‡‰ä½œç‚º invoke() çš„ç¬¬äºŒå€‹åƒæ•¸ï¼Œä¸æ˜¯ input dict çš„ä¸€éƒ¨åˆ†
        # recursion_limit æ§åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼ˆé˜²æ­¢ç„¡é™ loopï¼‰
        result = llm_engineer_with_tools.invoke(
            {"messages": [HumanMessage(content=request)]},
            {"recursion_limit": 100},
        )

        log.info(
            "â¬…ï¸ [Next Step] Engineer finished task. Returning result to Architect..."
        )

        # è¿”å› Engineer çš„æœ€çµ‚å›è¦†ï¼ˆtool loop çµæŸå¾Œçš„ AI messageï¼‰
        final_message = result["messages"][-1]
        return str(final_message.content)

    # NOTE: Architect agent æ“æœ‰ï¼š
    # - file tools (read_file, write_file, list_directory, etc.)
    # - refactor_code: å§”æ´¾é‡æ§‹ä»»å‹™çµ¦ Engineer
    # - generate_test: åŸ·è¡Œ characterization testing pipeline
    llm_architect_with_tools = create_agent(
        llm_architect,
        tools=tools + [refactor_code, generate_test],
        system_prompt=architect_system_prompt,
    )

    log.info(
        "Compiling LangGraph workflow (using create_agent with built-in tool loop)..."
    )

    # NOTE: ç›´æ¥è¿”å› architect agent graph
    # create_agent å·²ç¶“æ˜¯å®Œæ•´çš„ compiled graphï¼ŒåŒ…å«ï¼š
    # - agent node: å‘¼å« LLM
    # - tools node: åŸ·è¡Œ tool calls
    # - conditional edge: åˆ¤æ–·æ˜¯å¦ç¹¼çºŒ loop
    # ä¸éœ€è¦å†ç”¨ StateGraph åŒ…è£
    return llm_architect_with_tools


# =========================
# Runtime / main
# =========================


def parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:
    """Parses CLI arguments."""

    parser = argparse.ArgumentParser(
        description="Multi-agent refactoring orchestrator (LangGraph)."
    )

    parser.add_argument(
        "--config",
        type=str,
        default="/app/orchestrator/config.yaml",
        help="Path to YAML config (default: config.yaml)",
    )
    return parser.parse_args(argv)


def stream_pretty(app, user_input: str, log: LogPacker) -> None:
    """Streams the graph execution with basic formatting.

    NOTE: ä½¿ç”¨ create_agent å¾Œï¼Œæ‰€æœ‰ AI messages éƒ½ä¾†è‡ª Architect agentã€‚
    ç•¶ Architect å‘¼å« refactor_code tool æ™‚ï¼ŒEngineer çš„åŸ·è¡Œæ˜¯åŒæ­¥çš„ï¼Œ
    å…¶è¼¸å‡ºæœƒä½œç‚º tool result è¿”å›ï¼Œä¸æœƒç”¢ç”Ÿç¨ç«‹çš„ AI messageã€‚

    è¨Šæ¯æµç¨‹ï¼š
    1. AI (Architect) - å¯èƒ½åŒ…å« tool_calls
    2. Tool - tool åŸ·è¡Œçµæœï¼ˆåŒ…å« refactor_code/generate_test çš„è¼¸å‡ºï¼‰
    3. AI (Architect) - è™•ç† tool çµæœï¼Œå¯èƒ½ç¹¼çºŒå‘¼å« tools
    4. ... é‡è¤‡ç›´åˆ° Architect ä¸å†å‘¼å« tools

    Args:
        app: Compiled LangGraph application (from create_agent).
        user_input: User request prompt content.
        log: Logger wrapper.
    """

    inputs = {"messages": [HumanMessage(content=user_input)]}

    log.info(
        "ğŸš€ [Next Step] Injecting user input into the Graph and starting execution..."
    )

    # NOTE: è¿½è¹¤è¿­ä»£æ¬¡æ•¸ï¼Œç”¨æ–¼ debug
    iteration_count = 0

    for event in app.stream(inputs, {"recursion_limit": 100}, stream_mode="values"):
        iteration_count += 1
        last_msg = event["messages"][-1]

        if last_msg.type == "ai":
            # NOTE: æ‰€æœ‰ AI messages éƒ½ä¾†è‡ª Architectï¼ˆEngineer é€é tool åŸ·è¡Œï¼‰
            role = "Architect"

            if getattr(last_msg, "tool_calls", None):
                tool_names = [t.get("name", "") for t in last_msg.tool_calls]
                log.info(
                    f"ğŸ› ï¸ [Iteration {iteration_count}]\
                    {role} calling tools: {tool_names}"
                )

                # ç‰¹åˆ¥æ¨™è¨˜å§”æ´¾çµ¦ Engineer çš„æƒ…æ³
                if "refactor_code" in tool_names:
                    log.info("   â†³ Delegating refactoring task to Engineer...")
                if "generate_test" in tool_names:
                    log.info("   â†³ Triggering characterization testing pipeline...")
            else:
                log.info(f"ğŸ’¬ [Iteration {iteration_count}] {role} responding...")

            # è¼¸å‡º AI è¨Šæ¯å…§å®¹(æˆªæ–·éé•·å…§å®¹)
            content = str(last_msg.content)
            if len(content) > 500:
                log.info(
                    f"[{role}] {content[:500]}... \
                    (truncated, total {len(content)} chars)"
                )
            else:
                log.info(f"[{role}] {content}")

        elif last_msg.type == "tool":
            # Tool åŸ·è¡Œçµæœ
            tool_name = getattr(last_msg, "name", "unknown")
            content_len = len(str(last_msg.content))

            log.info(
                f"âœ… [Iteration {iteration_count}] Tool '{tool_name}' completed. "
                f"Output length: {content_len} chars"
            )

            # å°æ–¼é‡è¦ toolsï¼Œè¼¸å‡ºæ›´å¤šç´°ç¯€
            if tool_name in ("refactor_code", "generate_test"):
                content = str(last_msg.content)
                if len(content) > 300:
                    log.info(f"   â†³ Result preview: {content[:300]}...")
                else:
                    log.info(f"   â†³ Result: {content}")

        elif last_msg.type == "human":
            # åˆå§‹ user inputï¼ˆé€šå¸¸åªæœ‰ç¬¬ä¸€æ¬¡ï¼‰
            log.info(f"ğŸ‘¤ [Iteration {iteration_count}] User input received")

    log.info(f"ğŸ Graph execution completed after {iteration_count} iterations")


def main(argv: Sequence[str] | None = None) -> int:
    """Main entry point."""

    load_dotenv()
    print("ğŸ“‹ [Next Step] Loading configuration and arguments...")

    args = parse_args(argv)
    cfg = parse_app_config(Path(args.config).resolve())

    log = LogPacker(cfg.working_directory / cfg.log_filename)
    log.json(
        "config",
        {
            "working_directory": str(cfg.working_directory),
            "architect_model": cfg.architect.model,
            "engineer_model": cfg.engineer.model,
            "prompts": {
                "architect": str(cfg.prompts.architect_path),
                "engineer": str(cfg.prompts.engineer_path),
            },
        },
    )

    # è¨­å®š API ç«¯é»ç¶²å€
    ingest_url = cfg.ingest_url

    # è¨­å®šè¦å‚³é€çš„è³‡æ–™ (Payload)
    data = {
        "repo_url": cfg.repo_url,
        "start_prompt": "Start processing",
        "options": {},
        "save_path": "/workspace/init",
    }

    try:
        # ç™¼é€ POST è«‹æ±‚
        # ä½¿ç”¨ json= åƒæ•¸æœƒè‡ªå‹•å°‡å­—å…¸è½‰æ›ç‚º JSON å­—ä¸²ï¼Œä¸¦åŠ ä¸Šæ­£ç¢ºçš„ Header
        log.info(f"ğŸ“¡ [Ingestion] Sending request to {ingest_url}...")
        response = requests.post(ingest_url, json=data)

        # æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ (ç‹€æ…‹ç¢¼ç‚º 2xx)
        response.raise_for_status()

        # è¼¸å‡ºå›å‚³çµæœ
        log.info(f"âœ… [Ingestion] Success - Status: {response.status_code}")
        log.json("ingestion_response", response.json())

    except requests.exceptions.RequestException as e:
        # NOTE: Ingestion å¤±æ•—æ˜¯ critical errorï¼Œworkspace æœªåˆå§‹åŒ–
        # å¾ŒçºŒçš„ file operations æœƒå¤±æ•—ï¼Œå› æ­¤æ‡‰è©²ä¸­æ­¢åŸ·è¡Œ
        log.error(f"âŒ [Ingestion] Failed: {e}")
        log.error("Aborting workflow - workspace not initialized")
        return 1

    log.info("ğŸ§° [Next Step] Initializing file management tools & LLMs...")
    tools = init_file_management_tools(cfg, log)
    llm_architect, llm_engineer = init_llms(cfg, log)
    log.info("ğŸ—ï¸ [Next Step] Building the Agent Graph...")
    app = build_graph(cfg, tools, llm_architect, llm_engineer, log)
    # app.get_graph().print_ascii()
    user_input = render_user_input(cfg)
    log.info("â–¶ï¸ [Next Step] Starting multi-agent execution loop...")
    stream_pretty(app, user_input, log)
    log.info(f"ğŸ [Done] Execution finished. Check target directory: {cfg.target_dir}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
